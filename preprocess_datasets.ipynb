{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from utils import *\n",
    "from modules.lingo_json_file_creator import LingoJsonFileCreator\n",
    "from modules.drivegpt4_bddx_json_file_creator import DriveGPT4BDDXJsonFileCreator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BDD-X Paths\n",
    "BDD_X_DATASET_PATH = './datasets/bdd_x_dataset'\n",
    "BDD_X_TRAINING_VIDEOS_PATH = os.path.join(BDD_X_DATASET_PATH, 'train/videos')\n",
    "\n",
    "# LingoQA Paths\n",
    "LINGO_DATASET_PATH = './datasets/lingoqa_dataset'\n",
    "LINGO_ACTION_PATH = os.path.join(LINGO_DATASET_PATH, 'action')\n",
    "LINGO_SCENERY_PATH = os.path.join(LINGO_DATASET_PATH, 'scenery')\n",
    "LINGO_EVAL_PATH = os.path.join(LINGO_DATASET_PATH, 'evaluation')\n",
    "LINGO_IMAGES_PATH = os.path.join(LINGO_DATASET_PATH, 'images')\n",
    "LINGO_TRAIN_PATH = os.path.join(LINGO_DATASET_PATH, 'train')\n",
    "LINGO_VAL_PATH = os.path.join(LINGO_DATASET_PATH, 'val')\n",
    "\n",
    "# DriveGPT4 Paths\n",
    "DRIVE_BDDX_DATASET_PATH = './datasets/drivegpt4_dataset'\n",
    "DRIVE_BDDX_IMAGES_PATH = os.path.join(DRIVE_BDDX_DATASET_PATH, 'BDD_X_imgs_select')\n",
    "DRIVE_BDDX_VIDEOS_PATH = os.path.join(DRIVE_BDDX_DATASET_PATH, 'videos')\n",
    "\n",
    "\n",
    "# Our Paths\n",
    "OUR_DATASET_Path = './our_datasets'\n",
    "OUR_LINGO_DATASET_PATH = os.path.join(OUR_DATASET_Path, 'lingoqa_dataset')\n",
    "OUR_DRIVE_BDDX_DATASET_PATH = os.path.join(OUR_DATASET_Path, 'drivegpt4_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess LingoQA Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Action Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>segment_id</th>\n",
       "      <th>images</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>959d64df1f47dd115fb4ed14997106b3</td>\n",
       "      <td>a9f0e311b0c6f46a9cc7cb923234e60a</td>\n",
       "      <td>[images/train/a9f0e311b0c6f46a9cc7cb923234e60a...</td>\n",
       "      <td>What are you currently doing and why?</td>\n",
       "      <td>I am starting as the zebra crossing becomes cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6c6e29403bbfc6761b7d9eafbf6d3040</td>\n",
       "      <td>a9f0e311b0c6f46a9cc7cb923234e60a</td>\n",
       "      <td>[images/train/a9f0e311b0c6f46a9cc7cb923234e60a...</td>\n",
       "      <td>What are you paying attention to and why?</td>\n",
       "      <td>I am paying attention to the zebra crossing to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id                        segment_id  \\\n",
       "0  959d64df1f47dd115fb4ed14997106b3  a9f0e311b0c6f46a9cc7cb923234e60a   \n",
       "1  6c6e29403bbfc6761b7d9eafbf6d3040  a9f0e311b0c6f46a9cc7cb923234e60a   \n",
       "\n",
       "                                              images  \\\n",
       "0  [images/train/a9f0e311b0c6f46a9cc7cb923234e60a...   \n",
       "1  [images/train/a9f0e311b0c6f46a9cc7cb923234e60a...   \n",
       "\n",
       "                                    question  \\\n",
       "0      What are you currently doing and why?   \n",
       "1  What are you paying attention to and why?   \n",
       "\n",
       "                                              answer  \n",
       "0  I am starting as the zebra crossing becomes cl...  \n",
       "1  I am paying attention to the zebra crossing to...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_action = pd.read_parquet(os.path.join(LINGO_ACTION_PATH, 'train.parquet'))\n",
    "df_action.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUR_LINGO_DATASET_PATH, exist_ok=True)\n",
    "\n",
    "dataframe = df_action.groupby('segment_id')\n",
    "\n",
    "lingo_json_creator = LingoJsonFileCreator()\n",
    "json_data = lingo_json_creator.format_to_train(dataframe)\n",
    "lingo_json_creator.save_json(json_data, 'lingoqa_action.json', OUR_LINGO_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Code Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in the dataset: 265323\n",
      "Number of questions in the JSON file: 265323\n",
      "They have the same questions.\n",
      "\n",
      "Number of answers in the dataset: 265323\n",
      "Number of answers in the JSON file: 265323\n",
      "They have the same answers.\n",
      "\n",
      "Number of videos in the dataset: 24491\n",
      "Number of videos in the JSON file: 24491\n",
      "They have the same videos.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = os.path.join(OUR_LINGO_DATASET_PATH, 'lingoqa_action.json')\n",
    "\n",
    "questions_lst = df_action['question'].tolist()\n",
    "compare_num_questions(json_path, questions_lst)\n",
    "\n",
    "answers_lst = df_action['answer'].tolist()\n",
    "compare_num_answers(json_path, answers_lst)\n",
    "\n",
    "videos_lst = [video.replace('\\n', ',') for video in df_action['images'].astype(str).unique().tolist()]\n",
    "videos_lst = [ast.literal_eval(video)[0] for video in videos_lst]\n",
    "compare_num_videos(json_path, videos_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Scenery Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>segment_id</th>\n",
       "      <th>images</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e0c57fef6e10d9d2a813b14cae5c9517</td>\n",
       "      <td>355a614a2263d30aac9cd1505852c4dd</td>\n",
       "      <td>[images/train/355a614a2263d30aac9cd1505852c4dd...</td>\n",
       "      <td>What action are you currently taking as the dr...</td>\n",
       "      <td>I am turning left as the road weaves in that d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>088a0fcd840c8e7e545adfc550aa1d9b</td>\n",
       "      <td>355a614a2263d30aac9cd1505852c4dd</td>\n",
       "      <td>[images/train/355a614a2263d30aac9cd1505852c4dd...</td>\n",
       "      <td>Why are you taking this action?</td>\n",
       "      <td>It’s necessary to follow the curvature of the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id                        segment_id  \\\n",
       "0  e0c57fef6e10d9d2a813b14cae5c9517  355a614a2263d30aac9cd1505852c4dd   \n",
       "1  088a0fcd840c8e7e545adfc550aa1d9b  355a614a2263d30aac9cd1505852c4dd   \n",
       "\n",
       "                                              images  \\\n",
       "0  [images/train/355a614a2263d30aac9cd1505852c4dd...   \n",
       "1  [images/train/355a614a2263d30aac9cd1505852c4dd...   \n",
       "\n",
       "                                            question  \\\n",
       "0  What action are you currently taking as the dr...   \n",
       "1                    Why are you taking this action?   \n",
       "\n",
       "                                              answer  \n",
       "0  I am turning left as the road weaves in that d...  \n",
       "1  It’s necessary to follow the curvature of the ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scenery = pd.read_parquet(os.path.join(LINGO_SCENERY_PATH, 'train.parquet'))\n",
    "df_scenery.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUR_LINGO_DATASET_PATH, exist_ok=True)\n",
    "\n",
    "dataframe = df_scenery.groupby('segment_id')\n",
    "\n",
    "lingo_json_creator = LingoJsonFileCreator()\n",
    "json_data = lingo_json_creator.format_to_train(dataframe)\n",
    "lingo_json_creator.save_json(json_data, 'lingoqa_scenery.json', OUR_LINGO_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Code Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in the dataset: 148506\n",
      "Number of questions in the JSON file: 148506\n",
      "They have the same questions.\n",
      "\n",
      "Number of answers in the dataset: 148506\n",
      "Number of answers in the JSON file: 148506\n",
      "They have the same answers.\n",
      "\n",
      "Number of videos in the dataset: 3508\n",
      "Number of videos in the JSON file: 3508\n",
      "They have the same videos.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = os.path.join(OUR_LINGO_DATASET_PATH, 'lingoqa_scenery.json')\n",
    "\n",
    "questions_lst = df_scenery['question'].tolist()\n",
    "compare_num_questions(json_path, questions_lst)\n",
    "\n",
    "answers_lst = df_scenery['answer'].tolist()\n",
    "compare_num_answers(json_path, answers_lst)\n",
    "\n",
    "videos_lst = [video.replace('\\n', ',') for video in df_scenery['images'].astype(str).unique().tolist()]\n",
    "videos_lst = [ast.literal_eval(video)[0] for video in videos_lst]\n",
    "compare_num_videos(json_path, videos_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>segment_id</th>\n",
       "      <th>images</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1a938d25604410ccd63b60285919eaec</td>\n",
       "      <td>f2e4286e94457b8605069190e29f955a</td>\n",
       "      <td>[images/val/f2e4286e94457b8605069190e29f955a/0...</td>\n",
       "      <td>Is there a traffic light? If yes, what color i...</td>\n",
       "      <td>Yes, green.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1a938d25604410ccd63b60285919eaec</td>\n",
       "      <td>f2e4286e94457b8605069190e29f955a</td>\n",
       "      <td>[images/val/f2e4286e94457b8605069190e29f955a/0...</td>\n",
       "      <td>Is there a traffic light? If yes, what color i...</td>\n",
       "      <td>Yes, a temporary traffic light. It is showing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id                        segment_id  \\\n",
       "0  1a938d25604410ccd63b60285919eaec  f2e4286e94457b8605069190e29f955a   \n",
       "1  1a938d25604410ccd63b60285919eaec  f2e4286e94457b8605069190e29f955a   \n",
       "\n",
       "                                              images  \\\n",
       "0  [images/val/f2e4286e94457b8605069190e29f955a/0...   \n",
       "1  [images/val/f2e4286e94457b8605069190e29f955a/0...   \n",
       "\n",
       "                                            question  \\\n",
       "0  Is there a traffic light? If yes, what color i...   \n",
       "1  Is there a traffic light? If yes, what color i...   \n",
       "\n",
       "                                              answer  \n",
       "0                                        Yes, green.  \n",
       "1  Yes, a temporary traffic light. It is showing ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval = pd.read_parquet(os.path.join(LINGO_EVAL_PATH, 'val.parquet'))\n",
    "df_eval.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUR_LINGO_DATASET_PATH, exist_ok=True)\n",
    "\n",
    "lingo_json_creator = LingoJsonFileCreator()\n",
    "json_data = lingo_json_creator.format_to_evaluate(df_eval)\n",
    "lingo_json_creator.save_json(json_data, 'lingoqa_eval.json', OUR_LINGO_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Code Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in the dataset: 1000\n",
      "Number of questions in the JSON file: 1000\n",
      "They have the same questions.\n",
      "\n",
      "Number of answers in the dataset: 1000\n",
      "Number of answers in the JSON file: 1000\n",
      "They have the same answers.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m videos_lst \u001b[38;5;241m=\u001b[39m [video\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m df_eval[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist()]\n\u001b[1;32m     10\u001b[0m videos_lst \u001b[38;5;241m=\u001b[39m [ast\u001b[38;5;241m.\u001b[39mliteral_eval(video)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m videos_lst]\n\u001b[0;32m---> 11\u001b[0m \u001b[43mcompare_num_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideos_lst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/blockstorage/drivefusion_project/utils.py:91\u001b[0m, in \u001b[0;36mcompare_num_videos\u001b[0;34m(json_path, videos_lst, replace_this, is_eval)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m json_data:\n\u001b[0;32m---> 91\u001b[0m         videos_json\u001b[38;5;241m.\u001b[39madd(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(replace_this, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     94\u001b[0m num_videos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(videos_lst)\n\u001b[1;32m     95\u001b[0m num_videos_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(videos_json)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "json_path = os.path.join(OUR_LINGO_DATASET_PATH, 'lingoqa_eval.json')\n",
    "\n",
    "questions_lst = df_eval['question'].tolist()\n",
    "compare_num_questions(json_path, questions_lst, True)\n",
    "\n",
    "answers_lst = df_eval['answer'].tolist()\n",
    "compare_num_answers(json_path, answers_lst, True)\n",
    "\n",
    "videos_lst = [video.replace('\\n', ',') for video in df_eval['images'].astype(str).unique().tolist()]\n",
    "videos_lst = [ast.literal_eval(video)[0] for video in videos_lst]\n",
    "compare_num_videos(json_path, videos_lst, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess BDD-X DriveGPT4 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DRIVE_BDDX_DATASET_PATH, 'BDD_X_training_label.json'), 'r') as f:\n",
    "    drive_bddx_train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUR_DRIVE_BDDX_DATASET_PATH, exist_ok=True)\n",
    "\n",
    "drive_json_creator = DriveGPT4BDDXJsonFileCreator()\n",
    "json_data = drive_json_creator.format_to_train(drive_bddx_train_data)\n",
    "drive_json_creator.save_json(json_data, 'drivegpt_bddx_training.json', OUR_DRIVE_BDDX_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Code Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in the dataset: 114434\n",
      "Number of questions in the JSON file: 114434\n",
      "They have the same questions.\n",
      "\n",
      "Number of answers in the dataset: 114434\n",
      "Number of answers in the JSON file: 114434\n",
      "They have the same answers.\n",
      "\n",
      "Number of videos in the dataset: 14229\n",
      "Number of videos in the JSON file: 14229\n",
      "They have the same videos.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = os.path.join(OUR_DRIVE_BDDX_DATASET_PATH, 'drivegpt_bddx_training.json')\n",
    "\n",
    "questions_lst = []\n",
    "answers_lst = []\n",
    "for data in drive_bddx_train_data:\n",
    "    for conv in data['conversations']:\n",
    "        if conv['from'] == 'human':\n",
    "            questions_lst.append(conv['value'])\n",
    "        else:\n",
    "            answers_lst.append(conv['value'])\n",
    "\n",
    "compare_num_questions(json_path, questions_lst)\n",
    "compare_num_answers(json_path, answers_lst)\n",
    "\n",
    "# video_lst = df_action['images'].astype(str).unique().tolist()\n",
    "videos_original = set()\n",
    "for data in drive_bddx_train_data:\n",
    "    videos_original.add(data['id'])\n",
    "\n",
    "compare_num_videos(json_path, list(videos_original), replace_this='_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DRIVE_BDDX_DATASET_PATH, 'BDD_X_testing_label.json'), 'r') as f:\n",
    "    drive_bddx_test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUR_DRIVE_BDDX_DATASET_PATH, exist_ok=True)\n",
    "\n",
    "drive_json_creator = DriveGPT4BDDXJsonFileCreator()\n",
    "json_data = drive_json_creator.format_to_evaluate(drive_bddx_test_data)\n",
    "drive_json_creator.save_json(json_data, 'drivegpt_bddx_testing.json', OUR_DRIVE_BDDX_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Code Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in the dataset: 7244\n",
      "Number of questions in the JSON file: 7244\n",
      "They have the same questions.\n",
      "\n",
      "Number of answers in the dataset: 7244\n",
      "Number of answers in the JSON file: 7244\n",
      "They have the same answers.\n",
      "\n",
      "Number of videos in the dataset: 1811\n",
      "Number of videos in the JSON file: 1811\n",
      "They have the same videos.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = os.path.join(OUR_DRIVE_BDDX_DATASET_PATH, 'drivegpt_bddx_testing.json')\n",
    "\n",
    "questions_lst = []\n",
    "answers_lst = []\n",
    "for data in drive_bddx_test_data:\n",
    "    for conv in data['conversations']:\n",
    "        if conv['from'] == 'human':\n",
    "            questions_lst.append(conv['value'])\n",
    "        else:\n",
    "            answers_lst.append(conv['value'])\n",
    "\n",
    "compare_num_questions(json_path, questions_lst, True)\n",
    "compare_num_answers(json_path, answers_lst, True)\n",
    "\n",
    "\n",
    "videos_original = set()\n",
    "for data in drive_bddx_test_data:\n",
    "    videos_original.add(data['id'])\n",
    "\n",
    "compare_num_videos(json_path, list(videos_original), '_0.png', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess BDD-X Original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4590"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(BDD_X_DATASET_PATH, 'train.txt'), 'r') as f:\n",
    "    bdd_x_training_data = f.read().splitlines()\n",
    "    bdd_x_training_data = [line.split('_')[-1] for line in bdd_x_training_data]\n",
    "\n",
    "downloaded_videos = os.listdir(BDD_X_TRAINING_VIDEOS_PATH)\n",
    "downloaded_videos = [video.replace('.mov', '') for video in downloaded_videos]\n",
    "len(set(bdd_x_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1059"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(downloaded_videos).difference(set(bdd_x_training_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(BDD_X_DATASET_PATH, 'val.txt'), 'r') as f:\n",
    "    bdd_x_val_data = f.read().splitlines()\n",
    "    bdd_x_val_data = [line.split('_')[-1] for line in bdd_x_val_data]\n",
    "len(set(bdd_x_val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(bdd_x_val_data).intersection(set(downloaded_videos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(BDD_X_DATASET_PATH, 'test.txt'), 'r') as f:\n",
    "    bdd_x_test_data = f.read().splitlines()\n",
    "    bdd_x_test_data = [line.split('_')[-1] for line in bdd_x_test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(bdd_x_test_data).intersection(set(downloaded_videos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move videos to DriveGPT4 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DRIVE_BDDX_DATASET_PATH, 'BDD_X_training_label.json'), 'r') as f:\n",
    "    drive_bddx_train_data = json.load(f)\n",
    "    \n",
    "with open(os.path.join(DRIVE_BDDX_DATASET_PATH, 'BDD_X_testing_label.json'), 'r') as f:\n",
    "    drive_bddx_test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos in the training dataset: 3438\n",
      "Number of videos in the testing dataset: 502\n",
      "Number of the intersected videos between downloaded videos and videos in training dataset: 3438\n",
      "Number of the intersection videos between downloaded videos and videos in testing dataset: 502\n"
     ]
    }
   ],
   "source": [
    "videos_train_lst = set()\n",
    "for data in drive_bddx_train_data:\n",
    "    videos_train_lst.add(data['id'].split('_')[1])\n",
    "\n",
    "videos_test_lst = set()\n",
    "for data in drive_bddx_test_data:\n",
    "    videos_test_lst.add(data['id'].split('_')[1])\n",
    "    \n",
    "\n",
    "print(f'Number of videos in the training dataset: {len(videos_train_lst)}')\n",
    "print(f'Number of videos in the testing dataset: {len(videos_test_lst)}')\n",
    "print(f'Number of the intersected videos between downloaded videos and videos in training dataset: {len(set(downloaded_videos).intersection(videos_train_lst))}')\n",
    "print(f'Number of the intersection videos between downloaded videos and videos in testing dataset: {len(set(downloaded_videos).intersection(videos_test_lst))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the videos to move: 3940\n",
      "Moved 0 video\n",
      "Moved 100 video\n",
      "Moved 200 video\n",
      "Moved 300 video\n",
      "Moved 400 video\n",
      "Moved 500 video\n",
      "Moved 600 video\n",
      "Moved 700 video\n",
      "Moved 800 video\n",
      "Moved 900 video\n",
      "Moved 1000 video\n",
      "Moved 1100 video\n",
      "Moved 1200 video\n",
      "Moved 1300 video\n",
      "Moved 1400 video\n",
      "Moved 1500 video\n",
      "Moved 1600 video\n",
      "Moved 1700 video\n",
      "Moved 1800 video\n",
      "Moved 1900 video\n",
      "Moved 2000 video\n",
      "Moved 2100 video\n",
      "Moved 2200 video\n",
      "Moved 2300 video\n",
      "Moved 2400 video\n",
      "Moved 2500 video\n",
      "Moved 2600 video\n",
      "Moved 2700 video\n",
      "Moved 2800 video\n",
      "Moved 2900 video\n",
      "Moved 3000 video\n",
      "Moved 3100 video\n",
      "Moved 3200 video\n",
      "Moved 3300 video\n",
      "Moved 3400 video\n",
      "Moved 3500 video\n",
      "Moved 3600 video\n",
      "Moved 3700 video\n",
      "Moved 3800 video\n",
      "Moved 3900 video\n"
     ]
    }
   ],
   "source": [
    "videos_to_move = list(set(downloaded_videos).intersection(videos_train_lst).union(set(downloaded_videos).intersection(videos_test_lst)))\n",
    "print(f'Length of the videos to move: {len(videos_to_move)}')\n",
    "\n",
    "os.makedirs(DRIVE_BDDX_VIDEOS_PATH, exist_ok=True)\n",
    "\n",
    "for idx, video in enumerate(videos_to_move):\n",
    "    if idx % 100 == 0:\n",
    "        print(f'Moved {idx} video')\n",
    "    video_src_path = os.path.join(BDD_X_TRAINING_VIDEOS_PATH, video+'.mov')\n",
    "    video_dst_path = os.path.join(DRIVE_BDDX_VIDEOS_PATH, video+'.mov')\n",
    "    shutil.copy2(video_src_path, video_dst_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3940"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(DRIVE_BDDX_VIDEOS_PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
